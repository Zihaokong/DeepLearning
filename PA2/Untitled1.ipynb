{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_data, load_config, write_to_file, one_hot_encoding\n",
    "\n",
    "# Load configuration\n",
    "config = load_config('./config.yaml')\n",
    "\n",
    "# Load the data and reshape from (32 x 32) to (1024 x 1)\n",
    "x_train, y_train, x_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "label_0 = np.sum([1 if label == 0 else 0 for label in y_train])\n",
    "label_1 = np.sum([1 if label == 1 else 0 for label in y_train])\n",
    "label_2 = np.sum([1 if label == 2 else 0 for label in y_train])\n",
    "label_3 = np.sum([1 if label == 3 else 0 for label in y_train])\n",
    "label_4 = np.sum([1 if label == 4 else 0 for label in y_train])\n",
    "label_5 = np.sum([1 if label == 5 else 0 for label in y_train])\n",
    "label_6 = np.sum([1 if label == 6 else 0 for label in y_train])\n",
    "label_7 = np.sum([1 if label == 7 else 0 for label in y_train])\n",
    "label_8 = np.sum([1 if label == 8 else 0 for label in y_train])\n",
    "label_9 = np.sum([1 if label == 9 else 0 for label in y_train])\n",
    "\n",
    "print('Training set: ')\n",
    "print('Number of label 0:', label_0, 'Percentage', label_0 / len(y_train) * 100)\n",
    "print('Number of label 1:', label_1, 'Percentage', label_1 / len(y_train) * 100)\n",
    "print('Number of label 2:', label_2, 'Percentage', label_2 / len(y_train) * 100)\n",
    "print('Number of label 3:', label_3, 'Percentage', label_3 / len(y_train) * 100)\n",
    "print('Number of label 4:', label_4, 'Percentage', label_4 / len(y_train) * 100)\n",
    "print('Number of label 5:', label_5, 'Percentage', label_5 / len(y_train) * 100)\n",
    "print('Number of label 6:', label_6, 'Percentage', label_6 / len(y_train) * 100)\n",
    "print('Number of label 7:', label_7, 'Percentage', label_7 / len(y_train) * 100)\n",
    "print('Number of label 8:', label_8, 'Percentage', label_8 / len(y_train) * 100)\n",
    "print('Number of label 9:', label_9, 'Percentage', label_9 / len(y_train) * 100)\n",
    "\n",
    "# Dataset statistics\n",
    "label_0 = np.sum([1 if label == 0 else 0 for label in y_test])\n",
    "label_1 = np.sum([1 if label == 1 else 0 for label in y_test])\n",
    "label_2 = np.sum([1 if label == 2 else 0 for label in y_test])\n",
    "label_3 = np.sum([1 if label == 3 else 0 for label in y_test])\n",
    "label_4 = np.sum([1 if label == 4 else 0 for label in y_test])\n",
    "label_5 = np.sum([1 if label == 5 else 0 for label in y_test])\n",
    "label_6 = np.sum([1 if label == 6 else 0 for label in y_test])\n",
    "label_7 = np.sum([1 if label == 7 else 0 for label in y_test])\n",
    "label_8 = np.sum([1 if label == 8 else 0 for label in y_test])\n",
    "label_9 = np.sum([1 if label == 9 else 0 for label in y_test])\n",
    "\n",
    "print('Testing set: ')\n",
    "print('Number of label 0:', label_0, 'Percentage', label_0 / len(y_test) * 100)\n",
    "print('Number of label 1:', label_1, 'Percentage', label_1 / len(y_test) * 100)\n",
    "print('Number of label 2:', label_2, 'Percentage', label_2 / len(y_test) * 100)\n",
    "print('Number of label 3:', label_3, 'Percentage', label_3 / len(y_test) * 100)\n",
    "print('Number of label 4:', label_4, 'Percentage', label_4 / len(y_test) * 100)\n",
    "print('Number of label 5:', label_5, 'Percentage', label_5 / len(y_test) * 100)\n",
    "print('Number of label 6:', label_6, 'Percentage', label_6 / len(y_test) * 100)\n",
    "print('Number of label 7:', label_7, 'Percentage', label_7 / len(y_test) * 100)\n",
    "print('Number of label 8:', label_8, 'Percentage', label_8 / len(y_test) * 100)\n",
    "print('Number of label 9:', label_9, 'Percentage', label_9 / len(y_test) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "y_train = np.eye(len(y_train), 10)[y_train]\n",
    "y_test = np.eye(len(y_test), 10)[y_test]\n",
    "\n",
    "x_train = np.array([image.reshape((1024)) for image in x_train], dtype='float')\n",
    "x_test = np.array([image.reshape((1024)) for image in x_test], dtype='float')\n",
    "\n",
    "# Create validation set out of training data.\n",
    "num = int(len(x_train) * 0.8)\n",
    "[x_train, x_val]= np.split(x_train, [num])\n",
    "[y_train, y_val] = np.split(y_train, [num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_indices = np.random.choice(len(x_train), 6, replace=False)\n",
    "x_train_sample = x_train[sampling_indices, :]\n",
    "y_train_sample = y_train[sampling_indices, :]\n",
    "\n",
    "fig, axs = plt.subplots(2, 3)\n",
    "plt.subplots_adjust(hspace = 0.5)\n",
    "axs[0, 0].set_title('y = ' + str(int(np.argmax(y_train_sample[0]))))\n",
    "axs[0, 0].imshow(x_train_sample[0].reshape((32, 32)), cmap='gray')\n",
    "axs[0, 0].set_xticks([])\n",
    "axs[0, 1].set_title('y = ' + str(int(np.argmax(y_train_sample[1]))))\n",
    "axs[0, 1].imshow(x_train_sample[1].reshape((32, 32)), cmap='gray')\n",
    "axs[0, 1].set_xticks([])\n",
    "axs[0, 2].set_title('y = ' + str(int(np.argmax(y_train_sample[2]))))\n",
    "axs[0, 2].imshow(x_train_sample[2].reshape((32, 32)), cmap='gray')\n",
    "axs[0, 2].set_xticks([])\n",
    "axs[1, 0].set_title('y = ' + str(int(np.argmax(y_train_sample[3]))))\n",
    "axs[1, 0].imshow(x_train_sample[3].reshape((32, 32)), cmap='gray')\n",
    "axs[1, 0].set_xticks([])\n",
    "axs[1, 1].set_title('y = ' + str(int(np.argmax(y_train_sample[4]))))\n",
    "axs[1, 1].imshow(x_train_sample[4].reshape((32, 32)), cmap='gray')\n",
    "axs[1, 1].set_xticks([])\n",
    "axs[1, 2].set_title('y = ' + str(int(np.argmax(y_train_sample[5]))))\n",
    "axs[1, 2].imshow(x_train_sample[5].reshape((32, 32)), cmap='gray')\n",
    "axs[1, 2].set_xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature mean and standard deviation for x_train, and use them to\n",
    "# Z score x_train, X_val and X_test\n",
    "def z_score_train_test(train, val, test):\n",
    "    train_T = train.T\n",
    "    val_T = val.T\n",
    "    test_T = test.T\n",
    "    for i in range(len(train_T)):\n",
    "        mean = np.mean(train_T[i])\n",
    "        SD = np.std(train_T[i])\n",
    "        train_T[i] = (train_T[i] - mean) / SD\n",
    "        val_T[i] = (val_T[i] - mean) / SD\n",
    "        test_T[i] = (test_T[i] - mean) / SD\n",
    "    return train_T.T, val_T.T, test_T.T\n",
    "\n",
    "# Z-scoring\n",
    "x_train, x_val, x_test = z_score_train_test(x_train, x_val, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_indices = np.random.choice(len(x_train), 6, replace=False)\n",
    "x_train_sample = x_train[sampling_indices, :]\n",
    "y_train_sample = y_train[sampling_indices, :]\n",
    "\n",
    "fig, axs = plt.subplots(2, 3)\n",
    "plt.subplots_adjust(hspace = 0.5)\n",
    "axs[0, 0].set_title('y = ' + str(int(np.argmax(y_train_sample[0]))))\n",
    "axs[0, 0].imshow(x_train_sample[0].reshape((32, 32)), cmap='gray')\n",
    "axs[0, 0].set_xticks([])\n",
    "axs[0, 1].set_title('y = ' + str(int(np.argmax(y_train_sample[1]))))\n",
    "axs[0, 1].imshow(x_train_sample[1].reshape((32, 32)), cmap='gray')\n",
    "axs[0, 1].set_xticks([])\n",
    "axs[0, 2].set_title('y = ' + str(int(np.argmax(y_train_sample[2]))))\n",
    "axs[0, 2].imshow(x_train_sample[2].reshape((32, 32)), cmap='gray')\n",
    "axs[0, 2].set_xticks([])\n",
    "axs[1, 0].set_title('y = ' + str(int(np.argmax(y_train_sample[3]))))\n",
    "axs[1, 0].imshow(x_train_sample[3].reshape((32, 32)), cmap='gray')\n",
    "axs[1, 0].set_xticks([])\n",
    "axs[1, 1].set_title('y = ' + str(int(np.argmax(y_train_sample[4]))))\n",
    "axs[1, 1].imshow(x_train_sample[4].reshape((32, 32)), cmap='gray')\n",
    "axs[1, 1].set_xticks([])\n",
    "axs[1, 2].set_title('y = ' + str(int(np.argmax(y_train_sample[5]))))\n",
    "axs[1, 2].imshow(x_train_sample[5].reshape((32, 32)), cmap='gray')\n",
    "axs[1, 2].set_xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, t):\n",
    "    y = np.argmax(y, axis=1)\n",
    "    t = np.argmax(t, axis=1)\n",
    "    res = [y_hat == t_hat for y_hat, t_hat in zip(y, t)]\n",
    "    return np.sum(res) / len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "curr_loss = float('inf')\n",
    "prev_loss = float(\"inf\")\n",
    "patience = 5\n",
    "patience_streak = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralnet import *\n",
    "from copy import deepcopy\n",
    "\n",
    "model = NeuralNetwork(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Epoch 0 Loss 1.505270340943542 Accuracy 0.51999726999727\n",
      "[+] Epoch 1 Loss 1.1767364672311835 Accuracy 0.6432568932568933\n",
      "[+] Epoch 2 Loss 1.0349868272537028 Accuracy 0.6861861861861862\n",
      "[+] Epoch 3 Loss 0.9510741566772374 Accuracy 0.7121894621894622\n",
      "[+] Epoch 4 Loss 0.9125375181304797 Accuracy 0.7186049686049686\n",
      "[+] Epoch 5 Loss 0.8824518216073237 Accuracy 0.728023478023478\n",
      "[+] Epoch 6 Loss 0.8291013311087106 Accuracy 0.7469287469287469\n",
      "[+] Epoch 7 Loss 0.7911263474908584 Accuracy 0.7581217581217581\n",
      "[+] Epoch 8 Loss 0.7689896661076838 Accuracy 0.7648102648102648\n",
      "[+] Epoch 9 Loss 0.7476245722371461 Accuracy 0.7734780234780235\n",
      "[+] Epoch 10 Loss 0.7305109901473837 Accuracy 0.7787332787332787\n",
      "Epoch 11 Loss 0.75126887475221 Accuracy 0.76992901992902\n",
      "[+] Epoch 12 Loss 0.7301128706769006 Accuracy 0.7772317772317773\n",
      "[+] Epoch 13 Loss 0.7215497744146748 Accuracy 0.7776412776412777\n",
      "Epoch 14 Loss 0.7306637712853922 Accuracy 0.777027027027027\n",
      "[+] Epoch 15 Loss 0.6918628318453393 Accuracy 0.7914960414960415\n",
      "Epoch 16 Loss 0.6990843536724506 Accuracy 0.7864455364455365\n",
      "Epoch 17 Loss 0.6935434619256997 Accuracy 0.7881517881517881\n",
      "[+] Epoch 18 Loss 0.6733950542130938 Accuracy 0.7966830466830467\n",
      "Epoch 19 Loss 0.6823694403078355 Accuracy 0.7916325416325416\n",
      "Epoch 20 Loss 0.6807985832440299 Accuracy 0.7957957957957958\n",
      "[+] Epoch 21 Loss 0.6676560421120298 Accuracy 0.7943625443625444\n",
      "Epoch 22 Loss 0.6830210241141988 Accuracy 0.7912912912912913\n",
      "Epoch 23 Loss 0.6715481517946329 Accuracy 0.7952497952497952\n",
      "Epoch 24 Loss 0.6811084598991123 Accuracy 0.7915642915642915\n",
      "Epoch 25 Loss 0.6762271415065871 Accuracy 0.7954545454545454\n",
      "Epoch 26 Loss 0.6724688284447774 Accuracy 0.7987987987987988\n",
      "[+] Epoch 27 Loss 0.6461215114995873 Accuracy 0.8065110565110565\n",
      "Epoch 28 Loss 0.664963005333267 Accuracy 0.801051051051051\n",
      "Epoch 29 Loss 0.6644064718617578 Accuracy 0.8011193011193011\n"
     ]
    }
   ],
   "source": [
    "# Stochastic gradient descent (SGD)\n",
    "for i in range(config['epochs']):\n",
    "    # Randomize the order of the indices into the training set\n",
    "    shuffled_indices = np.random.permutation(len(x_train))\n",
    "    x_train = x_train[shuffled_indices]\n",
    "    y_train = y_train[shuffled_indices]\n",
    "    for j in range(0, len(x_train), config['batch_size']):\n",
    "        # Mini-batching\n",
    "        if (j + config['batch_size'] < len(x_train)):\n",
    "            batch_x = x_train[j : j + config['batch_size'], :]\n",
    "            batch_y = y_train[j : j + config['batch_size'], :]\n",
    "        else:\n",
    "            batch_x = x_train[[j, len(x_train) - 1]]\n",
    "            batch_y = y_train[[j, len(x_train) - 1]]\n",
    "        \n",
    "        model.forward(x=batch_x, targets=batch_y)\n",
    "        model.backward()\n",
    "        \n",
    "    y, curr_loss = model.forward(x=x_val, targets=y_val)\n",
    "    # Model performance evaluation\n",
    "    acc = accuracy(y, y_val)\n",
    "    # Best model\n",
    "    if curr_loss < best_loss:\n",
    "        best_loss = curr_loss\n",
    "        best_model = deepcopy(model)\n",
    "        print('[+] Epoch', i, 'Loss', curr_loss, 'Accuracy', acc)\n",
    "    else:\n",
    "        print('Epoch', i, 'Loss', curr_loss, 'Accuracy', acc)\n",
    "    # Early stop\n",
    "    if config['early_stop']:\n",
    "        if i > config['early_stop_epoch']:\n",
    "            if curr_loss >= best_loss:\n",
    "                patience_streak += 1\n",
    "            if patience_streak == 5:\n",
    "                print('Early stopped at epoch', i)\n",
    "                break\n",
    "    prev_loss = curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
