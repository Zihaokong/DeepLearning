{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_data, load_config, write_to_file, one_hot_encoding\n",
    "\n",
    "# Load configuration\n",
    "config = load_config('./config.yaml')\n",
    "\n",
    "# Load the data and reshape from (32 x 32) to (1024 x 1)\n",
    "x_train, y_train, x_test, y_test = load_data()\n",
    "\n",
    "# One-hot encoding\n",
    "y_train = np.eye(len(y_train), 10)[y_train]\n",
    "y_test = np.eye(len(y_test), 10)[y_test]\n",
    "\n",
    "x_train = np.array([image.reshape((1024)) for image in x_train], dtype='float')\n",
    "x_test = np.array([image.reshape((1024)) for image in x_test], dtype='float')\n",
    "\n",
    "# Create validation set out of training data.\n",
    "num = int(len(x_train) * 0.8)\n",
    "[x_train, x_val]= np.split(x_train, [num])\n",
    "[y_train, y_val] = np.split(y_train, [num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature mean and standard deviation for x_train, and use them to\n",
    "# Z score x_train, X_val and X_test\n",
    "def z_score_train_test(train, val, test):\n",
    "    train_T = train.T\n",
    "    val_T = val.T\n",
    "    test_T = test.T\n",
    "    for i in range(len(train_T)):\n",
    "        mean = np.mean(train_T[i])\n",
    "        SD = np.std(train_T[i])\n",
    "        train_T[i] = (train_T[i] - mean) / SD\n",
    "        val_T[i] = (val_T[i] - mean) / SD\n",
    "        test_T[i] = (test_T[i] - mean) / SD\n",
    "    return train_T.T, val_T.T, test_T.T\n",
    "\n",
    "# Z-scoring\n",
    "x_train, x_val, x_test = z_score_train_test(x_train, x_val, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "valid_acc = []\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# CSE 151B: Programming Assignment 2\n",
    "# Code snippet by Ajit Kumar, Savyasachi\n",
    "# Edited by Zihao Kong, Baichuan Wu\n",
    "# Fall 2020\n",
    "################################################################################\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class Activation:\n",
    "    \"\"\"\n",
    "    The class implements different types of activation functions for\n",
    "    your neural network layers.\n",
    "\n",
    "    Example (for sigmoid):\n",
    "        //>>> sigmoid_layer = Activation(\"sigmoid\")\n",
    "        //>>> z = sigmoid_layer(a)\n",
    "        //>>> gradient = sigmoid_layer.backward(delta=1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation_type=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        Initialize activation type and placeholders here.\n",
    "        \"\"\"\n",
    "        if activation_type not in [\"sigmoid\", \"tanh\", \"ReLU\"]:\n",
    "            raise NotImplementedError(\"%s is not implemented.\" % (activation_type))\n",
    "\n",
    "        # Type of non-linear activation.\n",
    "        self.activation_type = activation_type\n",
    "        # Placeholder for input. This will be used for computing gradients.\n",
    "        self.x = None\n",
    "\n",
    "    def __call__(self, a):\n",
    "        \"\"\"\n",
    "        This method allows your instances to be callable.\n",
    "        \"\"\"\n",
    "        return self.forward(a)\n",
    "\n",
    "    def forward(self, a):\n",
    "        \"\"\"\n",
    "        Compute the forward pass.\n",
    "        \"\"\"\n",
    "        if self.activation_type == \"sigmoid\":\n",
    "            return self.sigmoid(a)\n",
    "\n",
    "        elif self.activation_type == \"tanh\":\n",
    "            return self.tanh(a)\n",
    "\n",
    "        elif self.activation_type == \"ReLU\":\n",
    "            return self.ReLU(a)\n",
    "\n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        Compute the backward pass.\n",
    "        \"\"\"\n",
    "        if self.activation_type == \"sigmoid\":\n",
    "            grad = self.grad_sigmoid()\n",
    "\n",
    "        elif self.activation_type == \"tanh\":\n",
    "            grad = self.grad_tanh()\n",
    "\n",
    "        elif self.activation_type == \"ReLU\":\n",
    "            grad = self.grad_ReLU()\n",
    "\n",
    "        # Hadamard product\n",
    "        return grad * delta\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        if x >= 0:\n",
    "            return 1 / (1 + exp(-x))\n",
    "        else:\n",
    "            return exp(x) / (1 + exp(x))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        \"\"\"\n",
    "        Tanh activation function.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        \"\"\"\n",
    "        ReLU activation function.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        res = np.array(self.x)\n",
    "        res[res < 0] = 0\n",
    "        return res\n",
    "\n",
    "    def grad_sigmoid(self):\n",
    "        \"\"\"\n",
    "        Calculates Gradient of sigmoid activation function.\n",
    "        \"\"\"\n",
    "        return self.sigmoid(self.x) * (1 - self.sigmoid(self.x))\n",
    "\n",
    "    def grad_tanh(self):\n",
    "        \"\"\"\n",
    "        Calculates Gradient of tanh activation function.\n",
    "        \"\"\"\n",
    "        return 1 - self.tanh(self.x) ** 2\n",
    "\n",
    "    def grad_ReLU(self):\n",
    "        \"\"\"\n",
    "        Calculates Gradient of ReLU activation function.\n",
    "        \"\"\"\n",
    "        res = np.array(self.x)\n",
    "        res[res <= 0] = 0\n",
    "        res[res > 0] = 1\n",
    "        return res\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    This class implements Fully Connected layers for your neural network.\n",
    "\n",
    "    Example:\n",
    "        //>>> fully_connected_layer = Layer(1024, 100)\n",
    "        //>>> output = fully_connected_layer(input)\n",
    "        //>>> gradient = fully_connected_layer.backward(delta=1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_units, out_units):\n",
    "        \"\"\"\n",
    "        Define the architecture and create placeholder.\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        self.w = math.sqrt(2 / in_units) * np.random.randn(in_units, out_units) # Kaiming initialization\n",
    "        self.b = np.zeros((1, out_units))  # Create a placeholder for Bias\n",
    "        self.x = None  # Save the input to forward in this\n",
    "        self.a = None  # Save the output of forward pass in this (without activation)\n",
    "\n",
    "        self.d_x = None  # Save the gradient w.r.t x in this w_{jk}\n",
    "        self.d_w = None  # Save the gradient w.r.t w in this x_j\n",
    "        self.d_b = None  # Save the gradient w.r.t b in this\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Make layer callable.\n",
    "        \"\"\"\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the forward pass through the layer here.\n",
    "        Do not apply activation here.\n",
    "        Return self.a\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.a = self.x @ self.w + self.b\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        Write the code for backward pass. This takes in gradient from its next layer as input,\n",
    "        computes gradient for its weights and the delta to pass to its previous layers.\n",
    "        Return self.dx\n",
    "        \"\"\"\n",
    "        # \\frac{\\partial a_j}{\\partial w_{ij}} = x, \n",
    "        # where the gradient is - \\frac{\\partial E}{\\partial a_j} \\frac{a_j}{\\partial w_{ij}}\n",
    "        self.d_w = -1 * (self.x.T @ delta)\n",
    "\n",
    "        # derivative of bias is 1\n",
    "        self.d_b = -1 * np.ones((1, len(self.x))) @ delta\n",
    "\n",
    "        # derivative of input is the weighted sum of input of delta j and w_j\n",
    "        # delta is row major, change to column major first\n",
    "        self.d_x = (self.w @ delta.T).T\n",
    "\n",
    "        # propogate partial X to calculate last layer's delta\n",
    "        return self.d_x\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Create a Neural Network specified by the input configuration.\n",
    "\n",
    "    Example:\n",
    "        //>>> net = NeuralNetwork(config)\n",
    "        //>>> output = net(input)\n",
    "        //>>> net.backward()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Create the Neural Network using config.\n",
    "        \"\"\"\n",
    "        self.layers = []  # Store all layers in this list.\n",
    "        self.x = None  # Save the input to forward in this\n",
    "        self.y = None  # Save the output vector of model in this\n",
    "        self.targets = None  # Save the targets in forward in this variable\n",
    "        self.alpha = config['learning_rate'] # Save the learning rate from config\n",
    "        self.batch_size = config['batch_size'] #\n",
    "\n",
    "        # Add layers specified by layer_specs.\n",
    "        for i in range(len(config['layer_specs']) - 1):\n",
    "            self.layers.append(Layer(config['layer_specs'][i], config['layer_specs'][i + 1]))\n",
    "            if i < len(config['layer_specs']) - 2:\n",
    "                self.layers.append(Activation(config['activation']))\n",
    "\n",
    "    def __call__(self, x, targets=None):\n",
    "        \"\"\"\n",
    "        Make NeuralNetwork callable.\n",
    "        \"\"\"\n",
    "        return self.forward(x, targets)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"\n",
    "        Compute forward pass through all the layers in the network and return it.\n",
    "        If targets are provided, return loss as well.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.targets = targets\n",
    "        \n",
    "        temp = self.x\n",
    "        \n",
    "        for i in range(len(self.layers)):\n",
    "            # Calculate weighted sum of inputs / pass weighted sum through activation\n",
    "            temp = self.layers[i].forward(temp)\n",
    "\n",
    "        # activate ak to yk\n",
    "        self.y = self.softmax(temp)\n",
    "\n",
    "        # calculate loss if target is passed into the function\n",
    "        if targets is not None:\n",
    "            batch_loss = self.loss(self.y, self.targets)\n",
    "            return self.y, batch_loss\n",
    "        else:\n",
    "            return self.y\n",
    "\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Numerically stable softmax function\n",
    "        \"\"\"\n",
    "        row_max = np.amax(x, axis=1)\n",
    "        x = x - row_max.reshape(len(x), 1)\n",
    "\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1).reshape(-1, 1)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Implement backpropagation here.\n",
    "        Call backward methods of individual layer's.\n",
    "        \"\"\"\n",
    "        delta = [None] * len(self.layers)\n",
    "        delta[len(delta) - 1] = self.targets - self.y\n",
    "\n",
    "        # Backprop deltas\n",
    "        for i in reversed(range(len(self.layers) - 1)):\n",
    "            # Evaluate the delta term\n",
    "            delta[i] = self.layers[i + 1].backward(delta[i + 1])\n",
    "        self.layers[0].backward(delta[0])\n",
    "        \n",
    "        \n",
    "        # Update weights\n",
    "        for i in range(0, len(self.layers), 2):\n",
    "            \n",
    "            # w = [fan_in, fan_out] => x = [n, fan_in], delta = [n, fan_out], alpha = [1] => x.T @ delta * alpha\n",
    "        \n",
    "            self.layers[i].w = self.layers[i].w - 0.05 * self.layers[i].d_w / self.batch_size\n",
    "            # a = w_0 * b + w_1 * x1 ...\n",
    "            # b = [n, fan_out] => delta = [n, fan_out]\n",
    "            \n",
    "            self.layers[i].b = self.layers[i].b - 0.05 * self.layers[i].d_b\n",
    "\n",
    "    def loss(self, logits, targets):\n",
    "        \"\"\"\n",
    "        compute the categorical cross-entropy loss and return it.\n",
    "        \"\"\"\n",
    "        y_ylog = targets * np.log(logits + 1e-8)\n",
    "        return -1 * np.sum(y_ylog) / len(targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start epoch 0\n",
      "best loss detected: inf\n",
      "best loss detected: 2.718405281593034\n",
      "best loss detected: 2.6089785627758864\n",
      "best loss detected: 2.4348794767568767\n",
      "best loss detected: 2.43097788208903\n",
      "best loss detected: 2.4225464718919287\n",
      "best loss detected: 2.3605112310601237\n",
      "best loss detected: 2.339113873570516\n",
      "best loss detected: 2.307983394781702\n",
      "best loss detected: 2.2832598853385164\n",
      "best loss detected: 2.2335922582083683\n",
      "best loss detected: 2.113373068364999\n",
      "best loss detected: 2.0818371505255726\n",
      "best loss detected: 2.055216005680906\n",
      "best loss detected: 2.0405538078538905\n",
      "best loss detected: 1.9894407502282667\n",
      "best loss detected: 1.9128122160284704\n",
      "best loss detected: 1.8954551842864773\n",
      "best loss detected: 1.8846916420859472\n",
      "best loss detected: 1.8518335251148732\n",
      "best loss detected: 1.8477760485496901\n",
      "best loss detected: 1.847715518851536\n",
      "best loss detected: 1.813327525053683\n",
      "best loss detected: 1.7996086118366925\n",
      "best loss detected: 1.7975326660858406\n",
      "best loss detected: 1.7915273352825898\n",
      "best loss detected: 1.786455848929974\n",
      "best loss detected: 1.7089265116202381\n",
      "best loss detected: 1.6430230698992985\n",
      "best loss detected: 1.63846045365467\n",
      "best loss detected: 1.6024670374377346\n",
      "best loss detected: 1.5979893162521557\n",
      "best loss detected: 1.5570289799915322\n",
      "best loss detected: 1.5279529883393204\n",
      "best loss detected: 1.4863298054603453\n",
      "best loss detected: 1.4797922644861439\n",
      "best loss detected: 1.4646768795902527\n",
      "best loss detected: 1.4614825250423276\n",
      "best loss detected: 1.4523365110144781\n",
      "best loss detected: 1.4445827866515466\n",
      "best loss detected: 1.4242410005632027\n",
      "best loss detected: 1.3999485895625217\n",
      "start epoch 1\n",
      "best loss detected: 1.3170790715292051\n",
      "best loss detected: 1.3036495945091569\n",
      "best loss detected: 1.1970926168074272\n",
      "best loss detected: 1.1956243520008298\n",
      "best loss detected: 1.0929751539301589\n",
      "best loss detected: 1.0845362184002663\n",
      "best loss detected: 1.068374914389567\n",
      "best loss detected: 1.0476411676589854\n",
      "best loss detected: 1.030955741508447\n",
      "best loss detected: 1.0253215774476478\n",
      "best loss detected: 1.024996419795319\n",
      "best loss detected: 1.0110610819929882\n",
      "best loss detected: 0.994992745547543\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-eee191b7e308>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'early_stop_epoch'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"yes\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-6684d5a83c91>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    263\u001b[0m             \u001b[1;31m# w = [fan_in, fan_out] => x = [n, fan_in], delta = [n, fan_out], alpha = [1] => x.T @ delta * alpha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m0.05\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_w\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m             \u001b[1;31m# a = w_0 * b + w_1 * x1 ...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[1;31m# b = [n, fan_out] => delta = [n, fan_out]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "def accuracy(y, t):\n",
    "    y = np.argmax(y, axis=1)\n",
    "    t = np.argmax(t, axis=1)\n",
    "    res = [y_hat == t_hat for y_hat, t_hat in zip(y, t)]\n",
    "    return np.sum(res) / len(res)\n",
    "\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "curr_loss = float('inf')\n",
    "prev_loss = float(\"inf\")\n",
    "early_stop_mark = 0\n",
    "\n",
    "batch_index = 0\n",
    "\n",
    "model = NeuralNetwork(config=config)\n",
    "\n",
    "for i in range(config['epochs']): \n",
    "    print(\"start epoch\", i)\n",
    "    batch_index = 0\n",
    "    # Randomize the order of the indices into the training set\n",
    "    shuffled_indices = np.random.permutation(len(x_train))\n",
    "    x_train = x_train[shuffled_indices]\n",
    "    y_train = y_train[shuffled_indices]\n",
    "    for j in range(0, len(x_train), config['batch_size']):\n",
    "        batch_index += 1\n",
    "\n",
    "        if (j + config['batch_size'] < len(x_train)):\n",
    "            batch_x = x_train[j:j + config['batch_size'],:]\n",
    "            batch_y = y_train[j:j + config['batch_size'],:]\n",
    "        else:\n",
    "            batch_x = x_train[[j, len(x_train) - 1]]\n",
    "            batch_y = y_train[[j, len(x_train) - 1]]\n",
    "        \n",
    "        \n",
    "        y, curr_loss = model.forward(x=batch_x, targets=batch_y)\n",
    "        \n",
    "        if curr_loss <= best_loss:\n",
    "            print(\"best loss detected:\", best_loss)\n",
    "            accuracy(y,batch_y)\n",
    "            best_loss = curr_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "        \n",
    "        \n",
    "        model.backward()\n",
    "        \n",
    "        \n",
    "        #if early stop is true\n",
    "        if curr_loss >= prev_loss and batch_index > 100:\n",
    "            print(curr_loss,prev_loss)\n",
    "            print(\"increment counter\")\n",
    "            early_stop_mark += 1\n",
    "\n",
    "        if early_stop_mark == config['early_stop_epoch']:\n",
    "            print(\"Stop epoch \\n\\n\\n\",i)\n",
    "            early_stop_mark = 0\n",
    "            break\n",
    "        prev_loss = curr_loss\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    #y, loss = model.forward(x_val, y_val)\n",
    "    #acc = accuracy(y, y_val)\n",
    "    #print('Epoch', i, 'Loss', loss, 'Accuracy', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, loss = model.forward(x=x_val, targets=y_val)\n",
    "print(accuracy(y,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical approximation\n",
    "\n",
    "x = x_train[10:11,:]\n",
    "y = y_train[10:11,:]\n",
    "\n",
    "\n",
    "coorx = 0\n",
    "coory = 9\n",
    "layernum = 0\n",
    "model = NeuralNetwork(config=config)\n",
    "\n",
    "model.layers[layernum].b[coorx][coory] = model.layers[layernum].b[coorx][coory] + 0.001\n",
    "te, loss1 = model.forward(x,y)\n",
    "model.layers[layernum].b[coorx][coory] = model.layers[layernum].b[coorx][coory] - 0.002\n",
    "te, loss2 = model.forward(x,y)\n",
    "\n",
    "print(\"gradient expected\",(loss1-loss2)/(2*0.001))\n",
    "\n",
    "model.layers[layernum].b[coorx][coory] = model.layers[2].b[coorx][coory] + 0.001\n",
    "model.backward()\n",
    "\n",
    "print(\"gradient received\",model.layers[layernum].d_b[coorx][coory])#.d_w[coorx][coory]\n",
    "print(\"gradient diff\",model.layers[layernum].d_b[coorx][coory]-(loss1-loss2)/(2*0.001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNetwork(config=config)\n",
    "\n",
    "# # One output bias weight\n",
    "# epsilon = 1e-2\n",
    "# model.layers[2].b[0][8] += epsilon\n",
    "# y, loss = model.forward(x_val, targets=y_val)\n",
    "# e_w_plus = loss\n",
    "# model.layers[2].b[0][8] -= epsilon\n",
    "# y, loss = model.forward(x_val, targets=y_val)\n",
    "# e_w_minus = loss\n",
    "# e_w_diff = e_w_plus - e_w_minus\n",
    "# expected_d_w = (e_w_diff / 2 * epsilon)\n",
    "# model.backward()\n",
    "# actual_d_w = model.layers[2].d_b[0][8]\n",
    "# print('Expected', expected_d_w)\n",
    "# print('Actual', actual_d_w)\n",
    "# print('Agress?', np.absolute(expected_d_w - actual_d_w) < epsilon ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
