{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import load_data, load_config, write_to_file, one_hot_encoding\n",
    "\n",
    "# Load configuration\n",
    "config = load_config('./config.yaml')\n",
    "\n",
    "# Load the data and reshape from (32 x 32) to (1024 x 1)\n",
    "x_train, y_train, x_test, y_test = load_data()\n",
    "\n",
    "# One-hot encoding\n",
    "y_train = np.eye(len(y_train), 10)[y_train]\n",
    "y_test = np.eye(len(y_test), 10)[y_test]\n",
    "\n",
    "x_train = np.array([image.reshape((1024)) for image in x_train], dtype='float')\n",
    "x_test = np.array([image.reshape((1024)) for image in x_test], dtype='float')\n",
    "\n",
    "# Create validation set out of training data.\n",
    "num = int(len(x_train) * 0.8)\n",
    "[x_train, x_val]= np.split(x_train, [num])\n",
    "[y_train, y_val] = np.split(y_train, [num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature mean and standard deviation for x_train, and use them to\n",
    "# Z score x_train, X_val and X_test\n",
    "def z_score_train_test(train, val, test):\n",
    "    train_T = train.T\n",
    "    val_T = val.T\n",
    "    test_T = test.T\n",
    "    for i in range(len(train_T)):\n",
    "        mean = np.mean(train_T[i])\n",
    "        SD = np.std(train_T[i])\n",
    "        train_T[i] = (train_T[i] - mean) / SD\n",
    "        val_T[i] = (val_T[i] - mean) / SD\n",
    "        test_T[i] = (test_T[i] - mean) / SD\n",
    "    return train_T.T, val_T.T, test_T.T\n",
    "\n",
    "# Z-scoring\n",
    "x_train, x_val, x_test = z_score_train_test(x_train, x_val, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "valid_acc = []\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# CSE 151B: Programming Assignment 2\n",
    "# Code snippet by Ajit Kumar, Savyasachi\n",
    "# Edited by Zihao Kong, Baichuan Wu\n",
    "# Fall 2020\n",
    "################################################################################\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class Activation:\n",
    "    \"\"\"\n",
    "    The class implements different types of activation functions for\n",
    "    your neural network layers.\n",
    "\n",
    "    Example (for sigmoid):\n",
    "        //>>> sigmoid_layer = Activation(\"sigmoid\")\n",
    "        //>>> z = sigmoid_layer(a)\n",
    "        //>>> gradient = sigmoid_layer.backward(delta=1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation_type=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        Initialize activation type and placeholders here.\n",
    "        \"\"\"\n",
    "        if activation_type not in [\"sigmoid\", \"tanh\", \"ReLU\"]:\n",
    "            raise NotImplementedError(\"%s is not implemented.\" % (activation_type))\n",
    "\n",
    "        # Type of non-linear activation.\n",
    "        self.activation_type = activation_type\n",
    "        # Placeholder for input. This will be used for computing gradients.\n",
    "        self.x = None\n",
    "\n",
    "    def __call__(self, a):\n",
    "        \"\"\"\n",
    "        This method allows your instances to be callable.\n",
    "        \"\"\"\n",
    "        return self.forward(a)\n",
    "\n",
    "    def forward(self, a):\n",
    "        \"\"\"\n",
    "        Compute the forward pass.\n",
    "        \"\"\"\n",
    "        if self.activation_type == \"sigmoid\":\n",
    "            return self.sigmoid(a)\n",
    "\n",
    "        elif self.activation_type == \"tanh\":\n",
    "            return self.tanh(a)\n",
    "\n",
    "        elif self.activation_type == \"ReLU\":\n",
    "            return self.ReLU(a)\n",
    "\n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        Compute the backward pass.\n",
    "        \"\"\"\n",
    "        if self.activation_type == \"sigmoid\":\n",
    "            grad = self.grad_sigmoid()\n",
    "\n",
    "        elif self.activation_type == \"tanh\":\n",
    "            grad = self.grad_tanh()\n",
    "\n",
    "        elif self.activation_type == \"ReLU\":\n",
    "            grad = self.grad_ReLU()\n",
    "\n",
    "        # Hadamard product\n",
    "        return grad * delta\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        if x >= 0:\n",
    "            return 1 / (1 + exp(-x))\n",
    "        else:\n",
    "            return exp(x) / (1 + exp(x))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        \"\"\"\n",
    "        Tanh activation function.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        \"\"\"\n",
    "        ReLU activation function.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        res = np.array(self.x)\n",
    "        res[res < 0] = 0\n",
    "        return res\n",
    "\n",
    "    def grad_sigmoid(self):\n",
    "        \"\"\"\n",
    "        Calculates Gradient of sigmoid activation function.\n",
    "        \"\"\"\n",
    "        return self.sigmoid(self.x) * (1 - self.sigmoid(self.x))\n",
    "\n",
    "    def grad_tanh(self):\n",
    "        \"\"\"\n",
    "        Calculates Gradient of tanh activation function.\n",
    "        \"\"\"\n",
    "        return 1 - self.tanh(self.x) ** 2\n",
    "\n",
    "    def grad_ReLU(self):\n",
    "        \"\"\"\n",
    "        Calculates Gradient of ReLU activation function.\n",
    "        \"\"\"\n",
    "        res = np.array(self.x)\n",
    "        res[res <= 0] = 0\n",
    "        res[res > 0] = 1\n",
    "        return res\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    This class implements Fully Connected layers for your neural network.\n",
    "\n",
    "    Example:\n",
    "        //>>> fully_connected_layer = Layer(1024, 100)\n",
    "        //>>> output = fully_connected_layer(input)\n",
    "        //>>> gradient = fully_connected_layer.backward(delta=1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_units, out_units):\n",
    "        \"\"\"\n",
    "        Define the architecture and create placeholder.\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        self.w = math.sqrt(2 / in_units) * np.random.randn(in_units, out_units) # Kaiming initialization\n",
    "        self.b = np.zeros((1, out_units))  # Create a placeholder for Bias\n",
    "        self.x = None  # Save the input to forward in this\n",
    "        self.a = None  # Save the output of forward pass in this (without activation)\n",
    "\n",
    "        self.d_x = None  # Save the gradient w.r.t x in this w_{jk}\n",
    "        self.d_w = None  # Save the gradient w.r.t w in this x_j\n",
    "        self.d_b = None  # Save the gradient w.r.t b in this\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Make layer callable.\n",
    "        \"\"\"\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the forward pass through the layer here.\n",
    "        Do not apply activation here.\n",
    "        Return self.a\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.a = self.x @ self.w + self.b\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        Write the code for backward pass. This takes in gradient from its next layer as input,\n",
    "        computes gradient for its weights and the delta to pass to its previous layers.\n",
    "        Return self.dx\n",
    "        \"\"\"\n",
    "        # \\frac{\\partial a_j}{\\partial w_{ij}} = x, \n",
    "        # where the gradient is - \\frac{\\partial E}{\\partial a_j} \\frac{a_j}{\\partial w_{ij}}\n",
    "        self.d_w = -1 * (self.x.T @ delta)\n",
    "\n",
    "        # derivative of bias is 1\n",
    "        self.d_b = -1 * np.ones((1, len(self.x))) @ delta\n",
    "\n",
    "        # derivative of input is the weighted sum of input of delta j and w_j\n",
    "        # delta is row major, change to column major first\n",
    "        self.d_x = (self.w @ delta.T).T\n",
    "\n",
    "        # propogate partial X to calculate last layer's delta\n",
    "        return self.d_x\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Create a Neural Network specified by the input configuration.\n",
    "\n",
    "    Example:\n",
    "        //>>> net = NeuralNetwork(config)\n",
    "        //>>> output = net(input)\n",
    "        //>>> net.backward()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Create the Neural Network using config.\n",
    "        \"\"\"\n",
    "        self.layers = []  # Store all layers in this list.\n",
    "        self.x = None  # Save the input to forward in this\n",
    "        self.y = None  # Save the output vector of model in this\n",
    "        self.targets = None  # Save the targets in forward in this variable\n",
    "        self.alpha = config['learning_rate'] # Save the learning rate from config\n",
    "        self.batch_size = config['batch_size'] #\n",
    "\n",
    "        # Add layers specified by layer_specs.\n",
    "        for i in range(len(config['layer_specs']) - 1):\n",
    "            self.layers.append(Layer(config['layer_specs'][i], config['layer_specs'][i + 1]))\n",
    "            if i < len(config['layer_specs']) - 2:\n",
    "                self.layers.append(Activation(config['activation']))\n",
    "\n",
    "    def __call__(self, x, targets=None):\n",
    "        \"\"\"\n",
    "        Make NeuralNetwork callable.\n",
    "        \"\"\"\n",
    "        return self.forward(x, targets)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"\n",
    "        Compute forward pass through all the layers in the network and return it.\n",
    "        If targets are provided, return loss as well.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.targets = targets\n",
    "        \n",
    "        temp = self.x\n",
    "        \n",
    "        for i in range(len(self.layers)):\n",
    "            # Calculate weighted sum of inputs / pass weighted sum through activation\n",
    "            temp = self.layers[i].forward(temp)\n",
    "\n",
    "        # activate ak to yk\n",
    "        self.y = self.softmax(temp)\n",
    "\n",
    "        # calculate loss if target is passed into the function\n",
    "        if targets is not None:\n",
    "            batch_loss = self.loss(self.y, self.targets)\n",
    "            return self.y, batch_loss\n",
    "        else:\n",
    "            return self.y\n",
    "\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Numerically stable softmax function\n",
    "        \"\"\"\n",
    "        row_max = np.amax(x, axis=1)\n",
    "        x = x - row_max.reshape(len(x), 1)\n",
    "\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1).reshape(-1, 1)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Implement backpropagation here.\n",
    "        Call backward methods of individual layer's.\n",
    "        \"\"\"\n",
    "        delta = [None] * len(self.layers)\n",
    "        delta[len(delta) - 1] = self.targets - self.y\n",
    "\n",
    "        # Backprop deltas\n",
    "        for i in reversed(range(len(self.layers) - 1)):\n",
    "            # Evaluate the delta term\n",
    "            delta[i] = self.layers[i + 1].backward(delta[i + 1])\n",
    "        self.layers[0].backward(delta[0])\n",
    "        \n",
    "        \n",
    "        # Update weights\n",
    "        for i in range(0, len(self.layers), 2):\n",
    "            \n",
    "            # w = [fan_in, fan_out] => x = [n, fan_in], delta = [n, fan_out], alpha = [1] => x.T @ delta * alpha\n",
    "        \n",
    "            self.layers[i].w = self.layers[i].w - 0.05 * self.layers[i].d_w / self.batch_size\n",
    "            # a = w_0 * b + w_1 * x1 ...\n",
    "            # b = [n, fan_out] => delta = [n, fan_out]\n",
    "            \n",
    "            self.layers[i].b = self.layers[i].b - 0.05 * self.layers[i].d_b\n",
    "\n",
    "    def loss(self, logits, targets):\n",
    "        \"\"\"\n",
    "        compute the categorical cross-entropy loss and return it.\n",
    "        \"\"\"\n",
    "        y_ylog = targets * np.log(logits + 1e-8)\n",
    "        return -1 * np.sum(y_ylog) / len(targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start epoch 0\n",
      "best loss detected: inf\n",
      "accuracy:  51.999726999727 %\n",
      "start epoch 1\n",
      "best loss detected: 1.505270340943542\n",
      "accuracy:  64.32568932568932 %\n",
      "start epoch 2\n",
      "best loss detected: 1.1767364672311835\n",
      "accuracy:  68.61861861861863 %\n",
      "start epoch 3\n",
      "best loss detected: 1.0349868272537028\n",
      "accuracy:  71.21894621894623 %\n",
      "start epoch 4\n",
      "best loss detected: 0.9510741566772374\n",
      "accuracy:  71.86049686049685 %\n",
      "start epoch 5\n",
      "best loss detected: 0.9125375181304797\n",
      "accuracy:  72.8023478023478 %\n",
      "start epoch 6\n",
      "best loss detected: 0.8824518216073237\n",
      "accuracy:  74.69287469287468 %\n",
      "start epoch 7\n",
      "best loss detected: 0.8291013311087106\n",
      "accuracy:  75.81217581217581 %\n",
      "start epoch 8\n",
      "best loss detected: 0.7911263474908584\n",
      "accuracy:  76.48102648102648 %\n",
      "start epoch 9\n",
      "best loss detected: 0.7689896661076838\n",
      "accuracy:  77.34780234780236 %\n",
      "start epoch 10\n",
      "best loss detected: 0.7476245722371461\n",
      "accuracy:  77.87332787332787 %\n",
      "start epoch 11\n",
      "increment counter\n",
      "start epoch 12\n",
      "best loss detected: 0.7305109901473837\n",
      "accuracy:  77.72317772317773 %\n",
      "start epoch 13\n",
      "best loss detected: 0.7301128706769006\n",
      "accuracy:  77.76412776412776 %\n",
      "start epoch 14\n",
      "increment counter\n",
      "start epoch 15\n",
      "best loss detected: 0.7215497744146748\n",
      "accuracy:  79.14960414960414 %\n",
      "start epoch 16\n",
      "increment counter\n",
      "start epoch 17\n",
      "start epoch 18\n",
      "best loss detected: 0.6918628318453393\n",
      "accuracy:  79.66830466830467 %\n",
      "start epoch 19\n",
      "increment counter\n",
      "start epoch 20\n",
      "start epoch 21\n",
      "best loss detected: 0.6733950542130938\n",
      "accuracy:  79.43625443625444 %\n",
      "start epoch 22\n",
      "increment counter\n",
      "Stop epoch \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "def accuracy(y, t):\n",
    "    y = np.argmax(y, axis=1)\n",
    "    t = np.argmax(t, axis=1)\n",
    "    res = [y_hat == t_hat for y_hat, t_hat in zip(y, t)]\n",
    "    return np.sum(res) / len(res)\n",
    "\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "curr_loss = float('inf')\n",
    "prev_loss = float(\"inf\")\n",
    "early_stop_mark = 0\n",
    "\n",
    "batch_index = 0\n",
    "\n",
    "model = NeuralNetwork(config=config)\n",
    "\n",
    "for i in range(config['epochs']): \n",
    "    print(\"start epoch\", i)\n",
    "    batch_index = 0\n",
    "    # Randomize the order of the indices into the training set\n",
    "    shuffled_indices = np.random.permutation(len(x_train))\n",
    "    x_train = x_train[shuffled_indices]\n",
    "    y_train = y_train[shuffled_indices]\n",
    "    for j in range(0, len(x_train), config['batch_size']):\n",
    "        batch_index += 1\n",
    "\n",
    "        if (j + config['batch_size'] < len(x_train)):\n",
    "            batch_x = x_train[j:j + config['batch_size'],:]\n",
    "            batch_y = y_train[j:j + config['batch_size'],:]\n",
    "        else:\n",
    "            batch_x = x_train[[j, len(x_train) - 1]]\n",
    "            batch_y = y_train[[j, len(x_train) - 1]]\n",
    "        \n",
    "        \n",
    "        model.forward(x=batch_x, targets=batch_y)\n",
    "        model.backward()\n",
    "                \n",
    "    y, curr_loss = model.forward(x=x_val, targets=y_val)\n",
    "    if curr_loss <= best_loss:\n",
    "        print(\"best loss detected:\", best_loss)\n",
    "        print(\"accuracy: \",accuracy(y,y_val)*100,\"%\")\n",
    "        \n",
    "        best_loss = curr_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "         \n",
    "    \n",
    "    \n",
    "        \n",
    "    #if early stop is true\n",
    "    if curr_loss >= prev_loss:\n",
    "        print(\"increment counter\")\n",
    "        early_stop_mark += 1\n",
    "\n",
    "    if early_stop_mark == config['early_stop_epoch']:\n",
    "        print(\"Stop epoch \\n\\n\\n\")\n",
    "        early_stop_mark = 0\n",
    "        break\n",
    "    prev_loss = curr_loss\n",
    "    \n",
    "\n",
    "    \n",
    "    #y, loss = model.forward(x_val, y_val)\n",
    "    #acc = accuracy(y, y_val)\n",
    "    #print('Epoch', i, 'Loss', loss, 'Accuracy', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7723955132145052\n"
     ]
    }
   ],
   "source": [
    "y, loss = model.forward(x=x_test, targets=y_test)\n",
    "print(accuracy(y,y_test))\n",
    "\n",
    "\n",
    "pri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient expected -3.73e-02\n",
      "gradient received -3.71e-02\n",
      "gradient diff 2.30e-04\n"
     ]
    }
   ],
   "source": [
    "# numerical approximation\n",
    "\n",
    "x = x_train[10:11,:]\n",
    "y = y_train[10:11,:]\n",
    "\n",
    "\n",
    "coorx = 0\n",
    "coory = 8\n",
    "layernum = 0\n",
    "model = NeuralNetwork(config=config)\n",
    "epsilon = 1e-2\n",
    "\n",
    "model.layers[layernum].w[coorx][coory] = model.layers[layernum].w[coorx][coory] + epsilon\n",
    "te, loss1 = model.forward(x,y)\n",
    "model.layers[layernum].w[coorx][coory] = model.layers[layernum].w[coorx][coory] - 2 * epsilon\n",
    "te, loss2 = model.forward(x,y)\n",
    "\n",
    "print(\"gradient expected\", \"{:.2e}\".format((loss1-loss2)/(2 * epsilon)))\n",
    "\n",
    "model.layers[layernum].w[coorx][coory] = model.layers[2].w[coorx][coory] + epsilon\n",
    "model.backward()\n",
    "\n",
    "print(\"gradient received\", \"{:.2e}\".format(model.layers[layernum].d_w[coorx][coory])) #.d_w[coorx][coory]\n",
    "print(\"gradient diff\", \"{:.2e}\".format(model.layers[layernum].d_w[coorx][coory]-(loss1-loss2)/(2 * epsilon)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNetwork(config=config)\n",
    "\n",
    "# # One output bias weight\n",
    "# epsilon = 1e-2\n",
    "# model.layers[2].b[0][8] += epsilon\n",
    "# y, loss = model.forward(x_val, targets=y_val)\n",
    "# e_w_plus = loss\n",
    "# model.layers[2].b[0][8] -= epsilon\n",
    "# y, loss = model.forward(x_val, targets=y_val)\n",
    "# e_w_minus = loss\n",
    "# e_w_diff = e_w_plus - e_w_minus\n",
    "# expected_d_w = (e_w_diff / 2 * epsilon)\n",
    "# model.backward()\n",
    "# actual_d_w = model.layers[2].d_b[0][8]\n",
    "# print('Expected', expected_d_w)\n",
    "# print('Actual', actual_d_w)\n",
    "# print('Agress?', np.absolute(expected_d_w - actual_d_w) < epsilon ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
