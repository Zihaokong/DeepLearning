{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import model\n",
    "import datasets\n",
    "import ownmodel\n",
    "\n",
    "import copy\n",
    "# Make sure to use the GPU. The following line is just a check to see if GPU is availables\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'birds_dataset/train_list.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-174af732528e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# feel free to change header of bird_dataset class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'birds_dataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbird_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train_list.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbird_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'val_list.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_dataset1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbird_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train_list1.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CSE151BFA20/CSE151B_PA3/submission/datasets.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, file_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#open txt file and read files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'birds_dataset/train_list.txt'"
     ]
    }
   ],
   "source": [
    "# load your dataset and dataloader\n",
    "# feel free to change header of bird_dataset class\n",
    "root = './../birds_dataset'\n",
    "train_dataset = datasets.bird_dataset(root,'train_list.txt')\n",
    "val_dataset = datasets.bird_dataset(root,'val_list.txt')\n",
    "train_dataset1 = datasets.bird_dataset(root,'train_list1.txt')\n",
    "val_dataset1 = datasets.bird_dataset(root,'val_list1.txt')\n",
    "test_dataset = datasets.bird_dataset(root,'test_list.txt')\n",
    "\n",
    "#hyper param\n",
    "shuffle = True\n",
    "batch_size = 5\n",
    "drop_last = True\n",
    "learning_rate = 0.0001\n",
    "epoches = 25\n",
    "# Fill in optional arguments to the dataloader as you need it\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last)\n",
    "train_dataloader1 = DataLoader(train_dataset1,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=batch_size)\n",
    "val_dataloader1 = DataLoader(val_dataset1,batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create NN model object\n",
    "#baseline model\n",
    "#nn_model = model.baseline_Net(classes = 20)\n",
    "\n",
    "#self model\n",
    "\n",
    "\n",
    "\n",
    "nn_model = ownmodel.baseline_Net(classes = 20)\n",
    "\n",
    "#model to GPU\n",
    "nn_model = nn_model.to(device)\n",
    "\n",
    "# Create loss functions, optimizers\n",
    "# For baseline model use this\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate,weight_decay=0.001)\n",
    "\n",
    "# Xavier Initialize weights\n",
    "nn.init.xavier_uniform_(nn_model.fc1[0].weight)\n",
    "nn.init.zeros_(nn_model.fc1[0].bias)\n",
    "nn.init.xavier_uniform_(nn_model.fc2[0].weight)\n",
    "nn.init.zeros_(nn_model.fc2[0].bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 and Resnet18 transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "#uncomment and change variable name to run  resnet18\n",
    "\n",
    "# resnet18 = models.resnet18(pretrained = True)\n",
    "# for param in resnet18.parameters():\n",
    "#     param.requires_grad = True\n",
    "# resnet18.fc = nn.Linear(512,20)\n",
    "# nn.init.xavier_uniform_(resnet18.fc.weight)\n",
    "# nn.init.zeros_(resnet18.fc.bias)\n",
    "\n",
    "\n",
    "\n",
    "#uncomment to run vgg16\n",
    "\n",
    "# vgg16 = models.vgg16_bn(pretrained = True)\n",
    "# for param in vgg16.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# vgg16.classifier[6]=nn.Linear(4096,20)\n",
    "\n",
    "# nn.init.xavier_uniform_(vgg16.classifier[6].weight)\n",
    "# nn.init.zeros_(vgg16.classifier[6].bias)\n",
    "\n",
    "# # vgg16\n",
    "# nn_model = copy.deepcopy(vgg16)\n",
    "# nn_model = nn_model.to(device)\n",
    "# optimizer = torch.optim.Adam(nn_model.parameters(), lr=0.0001,weight_decay=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# valculate validatioin loss and validation accuracy\n",
    "def validation_loss(model,val_dataloader):\n",
    "    val_loss = 0\n",
    "    num = 0\n",
    "    correct = 0\n",
    "    total = len(val_dataloader.dataset)\n",
    "    # test it on validate set\n",
    "    model = model.cuda()\n",
    "    model.eval()\n",
    "    #softmax layer\n",
    "    soft = nn.Softmax(dim=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, label in val_dataloader:\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            out = model(img)\n",
    "            pred,ind = torch.max(soft(out),dim=1)\n",
    "            ind = ind.cpu()\n",
    "            lab = label.cpu()\n",
    "            correct += np.count_nonzero(ind==lab)\n",
    "            print(out.shape,label.shape)\n",
    "            val_loss += criterion(out,label)\n",
    "            num = num+1\n",
    "        accuracy = correct/total\n",
    "        val_loss = val_loss/num\n",
    "    return val_loss,accuracy\n",
    "\n",
    "# train your model\n",
    "# For each epoch iterate over your dataloaders/datasets, pass it to your NN model, get output, calculate loss and\n",
    "# backpropagate using optimizer\n",
    "# for epoch in range(epoches):\n",
    "\n",
    "x_train = []\n",
    "x_train.append(train_dataloader)\n",
    "x_train.append(train_dataloader1)\n",
    "x_val = []\n",
    "x_val.append(val_dataloader)\n",
    "x_val.append(val_dataloader1)\n",
    "\n",
    "best_loss = float(999999)\n",
    "\n",
    "train_loss_points = []\n",
    "val_loss_points = []\n",
    "train_accu_points = []\n",
    "val_accu_points = []\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "for i in range(epoches):\n",
    "    # initialize model, load into GPU\n",
    "    total_val_loss = 0\n",
    "    total_accuracy = 0\n",
    "    print(\"epoch: \",i)\n",
    "    # cross validation\n",
    "    train_len = len(x_train[0].dataset)\n",
    "    for i in range(len(x_train)):\n",
    "        train_correct = 0\n",
    "        train_loss = 0\n",
    "        num = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        nn_model.train(True)\n",
    "        \n",
    "        \n",
    "        nn_model = nn_model.cuda()\n",
    "        #for every train set, do mini batch learning\n",
    "        batchnum = 0\n",
    "        for image_batch,label_batch in x_train[i]:\n",
    "            # load minibatch, send to GPU\n",
    "            image_batch = image_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "            # gradient descent\n",
    "            outputs = nn_model(image_batch)\n",
    "\n",
    "            batchnum+=1\n",
    "            \n",
    "            pred,ind = torch.max(softmax(outputs),dim=1)\n",
    "            ind=ind.cpu()\n",
    "            labels = label_batch.cpu()\n",
    "            #number of correct label in a minibatch\n",
    "            train_correct+= np.count_nonzero(ind==labels)\n",
    "            \n",
    "            loss = criterion(outputs,label_batch)\n",
    "            #average minibatch's loss\n",
    "            train_loss += loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # increment batch loss and batch number\n",
    "            num+=1\n",
    "            \n",
    "        train_accu_points.append(train_correct/train_len)\n",
    "        train_loss_points.append(train_loss/num)\n",
    "        #plotting\n",
    "    \n",
    "        # test it on validate set\n",
    "        fold_val_loss,fold_val_accu = validation_loss(nn_model,x_val[i])\n",
    "        val_loss_points.append(fold_val_loss)\n",
    "        val_accu_points.append(fold_val_accu)\n",
    "        \n",
    "        print(\"train accu:\",train_correct/train_len)\n",
    "        total_accuracy += fold_val_accu\n",
    "        total_val_loss += fold_val_loss\n",
    "        \n",
    "    total_accuracy = total_accuracy/2\n",
    "    total_val_loss = total_val_loss/2\n",
    "    print(\"validation average loss\", total_val_loss.item(),\"accuracy:\",total_accuracy*100,\"%\")\n",
    "    if total_val_loss < best_loss:\n",
    "        print(\"best model\")\n",
    "        best_loss = total_val_loss\n",
    "        best_model = copy.deepcopy(nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "val_accu = []\n",
    "train_accu = []\n",
    "\n",
    "val_loss = []\n",
    "train_loss = []\n",
    "for i in range(0,len(val_accu_points)-1,2):\n",
    "    val_accu.append((val_accu_points[i] + val_accu_points[i+1])/2)\n",
    "    train_accu.append((train_accu_points[i] + train_accu_points[i+1])/2)\n",
    "    val_loss.append((val_loss_points[i]+val_loss_points[i+1])/2)\n",
    "    train_loss.append((train_loss_points[i]+train_loss_points[i+1])/2)\n",
    "\n",
    "plt.plot(val_accu,label=\"validation accuracy\")\n",
    "plt.plot(train_accu,label=\"train accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(\"resnet_accuracy.png\")\n",
    "\n",
    "plt.plot(train_loss,label=\"train loss\")\n",
    "plt.plot(val_loss,label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(\"resnet_loss.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accu = validation_loss(best_model,test_dataloader)\n",
    "print(\"test set accuracy\",accu*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scratch for weight map plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_kernels(tensor, num_cols=8):\n",
    "    if not tensor.ndim==4:\n",
    "        raise Exception(\"assumes a 4D tensor\")\n",
    "    if not tensor.shape[-1]==3:\n",
    "        raise Exception(\"last dim needs to be 3 to plot\")\n",
    "    num_kernels = tensor.shape[0]\n",
    "    num_rows = 1+ num_kernels // num_cols\n",
    "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
    "    for i in range(tensor.shape[0]):\n",
    "        print(tensor[i].shape)\n",
    "        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n",
    "        ax1.imshow(tensor[i])\n",
    "        ax1.axis('off')\n",
    "        ax1.set_xticklabels([])\n",
    "        ax1.set_yticklabels([])\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()\n",
    "    return \n",
    "  \n",
    "\n",
    "#filters = best_model.modules\n",
    "# vgg = models.vgg16(pretrained=True)\n",
    "# mm = vgg.double()\n",
    "# body_model = [i for i in mm.children()][0]\n",
    "# layer1 = body_model[0]\n",
    "# tensor = layer1.weight.data.numpy()\n",
    "\n",
    "best_model = best_model.cpu()\n",
    "\n",
    "layers = best_model.children()\n",
    "layer = None\n",
    "for i in layers:\n",
    "    layer = i\n",
    "    break\n",
    "#print(layer[0])\n",
    "tensor = layer[0].weight.data.numpy()\n",
    "print(tensor.shape)\n",
    "\n",
    "plot_kernels(tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
